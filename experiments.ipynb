{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from /shared-network/shared/2024_ml_master/data/mosaicml/mpt-7b/intervention_1_shots_max_20_words_further_templates_mpt2.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hydra\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import wandb\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from transformers import GPT2Tokenizer, BertTokenizer, AutoTokenizer, BloomTokenizerFast, GPTNeoXTokenizerFast, LlamaTokenizer\n",
    "from intervention_models.intervention_model import load_model\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import yaml\n",
    "from utils.number_utils import convert_to_words\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\" Dot notation access to dictionary attributes \"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "yaml_file_path = \"./conf/config.yaml\"\n",
    "with open(yaml_file_path, \"r\") as f:\n",
    "    args = DotDict(yaml.safe_load(f))\n",
    "\n",
    "file_name = args.data_dir\n",
    "file_name += '/' + str(args.model)\n",
    "\n",
    "n_shots = str(args.n_shots)\n",
    "max_n = str(args.max_n)\n",
    "representation = str(args.representation)\n",
    "file_name += '/intervention_' + n_shots + '_shots_' + 'max_' + max_n + '_' + representation\n",
    "file_name += '_further_templates' if args.extended_templates else ''\n",
    "file_name += '_mpt2' if args.mpt_data_version_2 else ''\n",
    "file_name += '.pkl'\n",
    "\n",
    "path_to_data = os.path.join(args.data_dir, file_name)\n",
    "with open(path_to_data, 'rb') as f:\n",
    "    intervention_list = pickle.load(f)\n",
    "print(\"Loaded data from\", path_to_data)\n",
    "if args.debug_run:\n",
    "    intervention_list = intervention_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aoq559/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/039e37745f00858f0e01e988383a8c4393b1a4f5/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n",
      "  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n",
      "/home/aoq559/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/039e37745f00858f0e01e988383a8c4393b1a4f5/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\n",
      "  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640f0dd2405d4960b979c9e868b44ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_device_map: {'transformer.wte': 0, 'transformer.emb_drop': 0, 'transformer.blocks.0': 0, 'transformer.blocks.1': 0, 'transformer.blocks.2': 0, 'transformer.blocks.3': 1, 'transformer.blocks.4': 1, 'transformer.blocks.5': 1, 'transformer.blocks.6': 1, 'transformer.blocks.7': 1, 'transformer.blocks.8': 2, 'transformer.blocks.9': 2, 'transformer.blocks.10': 2, 'transformer.blocks.11': 2, 'transformer.blocks.12': 2, 'transformer.blocks.13': 3, 'transformer.blocks.14': 3, 'transformer.blocks.15': 3, 'transformer.blocks.16': 3, 'transformer.blocks.17': 3, 'transformer.blocks.18': 4, 'transformer.blocks.19': 4, 'transformer.blocks.20': 4, 'transformer.blocks.21': 4, 'transformer.blocks.22': 4, 'transformer.blocks.23': 5, 'transformer.blocks.24': 5, 'transformer.blocks.25': 5, 'transformer.blocks.26': 5, 'transformer.blocks.27': 5, 'transformer.blocks.28': 6, 'transformer.blocks.29': 6, 'transformer.blocks.30': 6, 'transformer.blocks.31': 6, 'transformer.norm_f': 6}\n",
      "MPTConfig {\n",
      "  \"_name_or_path\": \"mosaicml/mpt-7b\",\n",
      "  \"architectures\": [\n",
      "    \"MPTForCausalLM\"\n",
      "  ],\n",
      "  \"attn_config\": {\n",
      "    \"alibi\": true,\n",
      "    \"alibi_bias_max\": 8,\n",
      "    \"attn_impl\": \"torch\",\n",
      "    \"attn_pdrop\": 0,\n",
      "    \"attn_type\": \"multihead_attention\",\n",
      "    \"attn_uses_sequence_id\": false,\n",
      "    \"clip_qkv\": null,\n",
      "    \"prefix_lm\": false,\n",
      "    \"qk_gn\": false,\n",
      "    \"qk_ln\": false,\n",
      "    \"rope\": false,\n",
      "    \"rope_dail_config\": {\n",
      "      \"pos_idx_in_fp32\": true,\n",
      "      \"type\": \"original\",\n",
      "      \"xpos_scale_base\": 512\n",
      "    },\n",
      "    \"rope_hf_config\": {\n",
      "      \"factor\": 1.0,\n",
      "      \"type\": \"no_scaling\"\n",
      "    },\n",
      "    \"rope_impl\": \"dail\",\n",
      "    \"rope_theta\": 10000,\n",
      "    \"sliding_window_size\": -1,\n",
      "    \"softmax_scale\": null\n",
      "  },\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"mosaicml/mpt-7b--configuration_mpt.MPTConfig\",\n",
      "    \"AutoModelForCausalLM\": \"mosaicml/mpt-7b--modeling_mpt.MPTForCausalLM\"\n",
      "  },\n",
      "  \"d_model\": 4096,\n",
      "  \"emb_pdrop\": 0,\n",
      "  \"embedding_fraction\": 1.0,\n",
      "  \"expansion_ratio\": 4,\n",
      "  \"fc_type\": \"torch\",\n",
      "  \"ffn_config\": {\n",
      "    \"fc_type\": \"torch\",\n",
      "    \"ffn_type\": \"mptmlp\"\n",
      "  },\n",
      "  \"init_config\": {\n",
      "    \"emb_init_std\": null,\n",
      "    \"emb_init_uniform_lim\": null,\n",
      "    \"fan_mode\": \"fan_in\",\n",
      "    \"init_div_is_residual\": true,\n",
      "    \"init_gain\": 0,\n",
      "    \"init_nonlinearity\": \"relu\",\n",
      "    \"init_std\": 0.02,\n",
      "    \"name\": \"kaiming_normal_\",\n",
      "    \"verbose\": 0\n",
      "  },\n",
      "  \"init_device\": \"cpu\",\n",
      "  \"learned_pos_emb\": false,\n",
      "  \"logit_scale\": null,\n",
      "  \"max_seq_len\": 2048,\n",
      "  \"model_type\": \"mpt\",\n",
      "  \"n_heads\": 32,\n",
      "  \"n_layers\": 32,\n",
      "  \"no_bias\": true,\n",
      "  \"norm_type\": \"low_precision_layernorm\",\n",
      "  \"resid_pdrop\": 0,\n",
      "  \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_pad_tok_in_ffn\": true,\n",
      "  \"verbose\": 0,\n",
      "  \"vocab_size\": 50432\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=load_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXTokenizerFast(name_or_path='mosaicml/mpt-7b', vocab_size=50254, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|padding|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50254: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50255: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50256: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50257: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50258: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50259: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50260: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50261: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50262: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50263: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50264: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50265: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50266: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50267: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50268: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50269: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50270: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50271: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50272: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50273: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50274: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50275: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50276: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "}\n",
      "['a', 'Ä 14']\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mosaicml/mpt-7b\")\n",
    "print(tokenizer)\n",
    "print(tokenizer.tokenize(\"a 14\", add_special_tokens=False))\n",
    "print(tokenizer.decode([204592]))\n",
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "intervention_data = []\n",
    "for intervention in intervention_list:\n",
    "    intervention_data.append(intervention.__dict__)\n",
    "\n",
    "df = pd.DataFrame(intervention_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['op3_pos', 'operator_word', 'operands_alt', 'operands_base',\n",
       "       'operator_pos', 'op2_pos', 'op1_pos', 'res_alt_tok', 'res_base_tok',\n",
       "       'res_string', 'res_base_string', 'res_alt_string', 'device',\n",
       "       'multitoken', 'is_llama', 'is_opt', 'is_bloom', 'is_mistral',\n",
       "       'is_persimmon', 'representation', 'extended_templates', 'template_id',\n",
       "       'n_vars', 'base_string', 'alt_string', 'few_shots', 'few_shots_t2',\n",
       "       'equation', 'enc', 'len_few_shots', 'len_few_shots_t2',\n",
       "       'base_string_tok_list', 'alt_string_tok_list', 'base_string_tok',\n",
       "       'alt_string_tok', 'pred_alt_string', 'pred_res_alt_tok'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equation\n",
      "({x} * {y} * {z})    25\n",
      "({x}*{y} * {z})      24\n",
      "({x}*{y}*{z})        21\n",
      "({x}+{y}+{z})        11\n",
      "({x}+{y} + {z})      11\n",
      "(({x}-{y})*{z})       5\n",
      "({x} -{y}-{z})        3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "equation_counts = df['equation'].value_counts()\n",
    "print(equation_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thirteen', 'seven', '', 'ten', 'eight', 'ten', 'fifteen', 'ten', 'fourteen', 'thirteen', 'nine', 'twelve', '', 'fourteen', '', '13', '13', 'seven', 'fifteen', 'nine', '', 'six', '', '', 'six', 'eight', '', '13', 'ten', '14', '13', '14', '12', 'eleven', 'six', '14', 'five', 'six', 'thirteen', 'seven', 'fifteen', 'thirteen', 'five', '3', '3', '6', '4', '6', '6', '6', '6', '6', '8', '6', '6', '6', '8', '3', '4', '8', '6', '3', '8', '6', '6', '6', '4', '6', '12', '12', '8', '12', '6', '6', '6', '9', '8', '8', '8', '6', '8', '6', '8', '8', '6', '6', '8', '9', 'four', 'two', 'two', 'two', 'two', 'four', 'two', 'six', 'two', 'six', 'four', 'six', 'two', 'twenty', 'twenty', '-', '(', 'twenty', '-', 'thirty', 'thirty', 'twenty', '18']\n"
     ]
    }
   ],
   "source": [
    "pred_alt_string = [intervention.pred_alt_string[1:] for intervention in intervention_list]\n",
    "print(pred_alt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nineteen', 'eleven', 'eighteen', 'fourteen', 'twelve', 'fourteen', 'nineteen', 'fourteen', 'sixteen', 'nineteen', 'eleven', 'eighteen', 'fifteen', 'sixteen', 'eighteen', 'fifteen', 'fifteen', 'nine', 'nineteen', 'eleven', 'eighteen', 'ten', 'seventeen', 'sixteen', 'nine', 'ten', 'eighteen', 'seventeen', 'twelve', 'sixteen', 'fifteen', 'sixteen', 'fourteen', 'thirteen', 'eight', 'eighteen', 'one', 'four', 'three', 'eight', 'five', 'eleven', 'three', 'twelve', 'twelve', 'eighteen', 'twelve', 'twelve', 'eighteen', 'eighteen', 'twelve', 'eighteen', 'sixteen', 'eighteen', 'twelve', 'eighteen', 'sixteen', 'twelve', 'sixteen', 'sixteen', 'eighteen', 'twelve', 'sixteen', 'twelve', 'twelve', 'eighteen', 'sixteen', 'twelve', 'eighteen', 'eighteen', 'eight', 'eighteen', 'twelve', 'twelve', 'twelve', 'eighteen', 'sixteen', 'sixteen', 'eight', 'twelve', 'eight', 'twelve', 'sixteen', 'eight', 'twelve', 'twelve', 'eight', 'eighteen', 'twelve', 'twelve', 'eight', 'eight', 'twelve', 'twelve', 'eight', 'eighteen', 'eight', 'eighteen', 'twelve', 'eighteen', 'twelve', 'twelve', 'two', 'three', 'sixteen', 'two', 'six', 'eighteen', 'seventeen', 'fourteen', 'six']\n"
     ]
    }
   ],
   "source": [
    "res_alt_string = [intervention.res_alt_string for intervention in intervention_list]\n",
    "print(res_alt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nineteen', 'eleven', 'eighteen', 'fourteen', 'twelve', 'fourteen', 'nineteen', 'fourteen', 'sixteen', 'nineteen', 'eleven', 'eighteen', 'fifteen', 'sixteen', 'eighteen', 'fifteen', 'fifteen', 'nine', 'nineteen', 'eleven', 'eighteen', 'ten', 'seventeen', 'sixteen', 'nine', 'ten', 'eighteen', 'seventeen', 'twelve', 'sixteen', 'fifteen', 'sixteen', 'fourteen', 'thirteen', 'eight', 'eighteen', 'one', 'four', 'three', 'eight', 'five', 'eleven', 'three', 'twelve', 'twelve', 'eighteen', 'twelve', 'twelve', 'eighteen', 'eighteen', 'twelve', 'eighteen', 'sixteen', 'eighteen', 'twelve', 'eighteen', 'sixteen', 'twelve', 'sixteen', 'sixteen', 'eighteen', 'twelve', 'sixteen', 'twelve', 'twelve', 'eighteen', 'sixteen', 'twelve', 'eighteen', 'eighteen', 'eight', 'eighteen', 'twelve', 'twelve', 'twelve', 'eighteen', 'sixteen', 'sixteen', 'eight', 'twelve', 'eight', 'twelve', 'sixteen', 'eight', 'twelve', 'twelve', 'eight', 'eighteen', 'twelve', 'twelve', 'eight', 'eight', 'twelve', 'twelve', 'eight', 'eighteen', 'eight', 'eighteen', 'twelve', 'eighteen', 'twelve', 'twelve', 'two', 'three', 'sixteen', 'two', 'six', 'eighteen', 'seventeen', 'fourteen', 'six']\n"
     ]
    }
   ],
   "source": [
    "res_base_string = [intervention.res_base_string for intervention in intervention_list]\n",
    "print(res_base_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "equal = [pred_alt == res_base for pred_alt, res_base in zip(pred_alt_string, res_base_string)]\n",
    "print(np.mean(equal))\n",
    "print(equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gpt_neox to instantiate a model of type gpt_neo. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/transformer/lm-arithmetic/intervention_models/intervention_model.py:22\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(args):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmodel_ckpt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_ckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformers_cache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformers_cache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint8\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/transformer/lm-arithmetic/intervention_models/intervention_model.py:80\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, device, random_weights, model_version, model_ckpt, transformers_cache_dir, int8)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#device_map={'model.decoder.embed_tokens': 0, 'lm_head': 0, 'model.decoder.embed_positions': 0, 'model.decoder.final_layer_norm': 0, 'model.decoder.layers.0': 0, 'model.decoder.layers.1': 0, 'model.decoder.layers.2': 0, 'model.decoder.layers.3': 1, 'model.decoder.layers.4': 1, 'model.decoder.layers.5': 1, 'model.decoder.layers.6': 1, 'model.decoder.layers.7': 1, 'model.decoder.layers.8': 1, 'model.decoder.layers.9': 1, 'model.decoder.layers.10': 2, 'model.decoder.layers.11': 2, 'model.decoder.layers.12': 2, 'model.decoder.layers.13': 2, 'model.decoder.layers.14': 2, 'model.decoder.layers.15': 2, 'model.decoder.layers.16': 2, 'model.decoder.layers.17': 3, 'model.decoder.layers.18': 3, 'model.decoder.layers.19': 3, 'model.decoder.layers.20': 3, 'model.decoder.layers.21': 3, 'model.decoder.layers.22': 3, 'model.decoder.layers.23': 3, 'model.decoder.layers.24': 5, 'model.decoder.layers.25': 5, 'model.decoder.layers.26': 5, 'model.decoder.layers.27': 5, 'model.decoder.layers.28': 5, 'model.decoder.layers.29': 5, 'model.decoder.layers.30': 5, 'model.decoder.layers.31': 6, 'model.decoder.layers.32': 6, 'model.decoder.layers.33': 6, 'model.decoder.layers.34': 6, 'model.decoder.layers.35': 6, 'model.decoder.layers.36': 6, 'model.decoder.layers.37': 6, 'model.decoder.layers.38': 7, 'model.decoder.layers.39': 7, 'model.decoder.layers.40': 7, 'model.decoder.layers.41': 7, 'model.decoder.layers.42': 7, 'model.decoder.layers.43': 7, 'model.decoder.layers.44': 7, 'model.decoder.layers.45': 7, 'model.decoder.layers.46': 7, 'model.decoder.layers.47': 7}, #'balanced', #'auto',\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformers_cache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_device_map\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_device_map:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhf_device_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/arithmetic/lib/python3.10/site-packages/transformers/modeling_utils.py:2611\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2608\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2611\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[38;5;66;03m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m~/miniconda3/envs/arithmetic/lib/python3.10/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:674\u001b[0m, in \u001b[0;36mGPTNeoForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 674\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mGPTNeoModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/arithmetic/lib/python3.10/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:484\u001b[0m, in \u001b[0;36mGPTNeoModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mmax_position_embeddings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;28mfloat\u001b[39m(config\u001b[38;5;241m.\u001b[39membed_dropout))\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([GPTNeoBlock(config, layer_id\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_layers)])\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/arithmetic/lib/python3.10/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:484\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mmax_position_embeddings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;28mfloat\u001b[39m(config\u001b[38;5;241m.\u001b[39membed_dropout))\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mGPTNeoBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_layers)])\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/arithmetic/lib/python3.10/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:313\u001b[0m, in \u001b[0;36mGPTNeoBlock.__init__\u001b[0;34m(self, config, layer_id)\u001b[0m\n\u001b[1;32m    311\u001b[0m inner_dim \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mintermediate_size \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mintermediate_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m hidden_size\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m \u001b[43mGPTNeoAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m GPTNeoMLP(inner_dim, config)\n",
      "File \u001b[0;32m~/miniconda3/envs/arithmetic/lib/python3.10/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:261\u001b[0m, in \u001b[0;36mGPTNeoAttention.__init__\u001b[0;34m(self, config, layer_id)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id \u001b[38;5;241m=\u001b[39m layer_id\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layers \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mattention_layers\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m GPTNeoSelfAttention(config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_type)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model = load_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35afc5fb214148c387b8044ff531d59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_device_map: {'gpt_neox.embed_in': 0, 'gpt_neox.layers.0': 0, 'gpt_neox.layers.1': 0, 'gpt_neox.layers.2': 0, 'gpt_neox.layers.3': 0, 'gpt_neox.layers.4': 1, 'gpt_neox.layers.5': 1, 'gpt_neox.layers.6': 1, 'gpt_neox.layers.7': 1, 'gpt_neox.layers.8': 1, 'gpt_neox.layers.9': 2, 'gpt_neox.layers.10': 2, 'gpt_neox.layers.11': 2, 'gpt_neox.layers.12': 2, 'gpt_neox.layers.13': 2, 'gpt_neox.layers.14': 3, 'gpt_neox.layers.15': 3, 'gpt_neox.layers.16': 3, 'gpt_neox.layers.17': 3, 'gpt_neox.layers.18': 3, 'gpt_neox.layers.19': 4, 'gpt_neox.layers.20': 4, 'gpt_neox.layers.21': 4, 'gpt_neox.layers.22': 4, 'gpt_neox.layers.23': 4, 'gpt_neox.layers.24': 5, 'gpt_neox.layers.25': 5, 'gpt_neox.layers.26': 5, 'gpt_neox.layers.27': 5, 'gpt_neox.layers.28': 5, 'gpt_neox.layers.29': 6, 'gpt_neox.layers.30': 6, 'gpt_neox.layers.31': 6, 'gpt_neox.layers.32': 6, 'gpt_neox.layers.33': 6, 'gpt_neox.layers.34': 7, 'gpt_neox.layers.35': 7, 'gpt_neox.final_layer_norm': 7, 'embed_out': 7}\n"
     ]
    }
   ],
   "source": [
    "model = load_model(args)\n",
    "tokenizer_class = (GPT2Tokenizer if model.is_gpt2 or model.is_gptneo or model.is_opt else\n",
    "                       BertTokenizer if model.is_bert else\n",
    "                       AutoTokenizer if model.is_gptj or model.is_flan or model.is_pythia else\n",
    "                       BloomTokenizerFast if model.is_bloom else\n",
    "                       GPTNeoXTokenizerFast if model.is_neox else\n",
    "                       LlamaTokenizer if model.is_llama else\n",
    "                       None)\n",
    "if not tokenizer_class:\n",
    "    raise Exception(f'Tokenizer for model {args.model} not found')\n",
    "if 'goat' in args.model:\n",
    "    tokenizer_id = 'decapoda-research/llama-7b-hf'\n",
    "else:\n",
    "    tokenizer_id = args.model\n",
    "tokenizer = tokenizer_class.from_pretrained(tokenizer_id, cache_dir=args.transformers_cache_dir)\n",
    "model.create_vocab_subset(tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXAttention(\n",
       "  (rotary_emb): RotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=5120, out_features=15360, bias=True)\n",
       "  (dense): Linear(in_features=5120, out_features=5120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_0 = model.get_hidden_states(1).attention\n",
    "layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<interventions.Intervention object at 0x7faba81201f0>\n",
      "9\n",
      "four * two * two = sixteen. \n",
      "({x}*{y}*{z})\n",
      "tensor([12496,   475,   767,   475,   767,   426, 25279,    15,   767,   475,\n",
      "          767,   475,  1264,   426])\n",
      "four * two * two = sixteen. two * two * three =\n"
     ]
    }
   ],
   "source": [
    "intervention = intervention_list[100]\n",
    "print(intervention)\n",
    "print(intervention.len_few_shots)\n",
    "print(intervention.few_shots)\n",
    "print(intervention.equation)\n",
    "print(intervention.base_string_tok[0])\n",
    "print(tokenizer.decode(intervention.base_string_tok[0][:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10\n",
      "884\n",
      "four * two * two = 10.\n",
      "[0, 2, 4]\n",
      " z * b * number = 10. two * two * three =\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "equation_position_operands={\"({x}+{y}+{z})\": [0, 2, 4],\n",
    "\"({x}+{y} + {z})\": [3, 5, 7], \n",
    "\"({x} -{y}-{z})\": [3, 5, 7], \n",
    "\"({x}*{y} * {z})\": [3, 5, 7], \n",
    "\"({x} * {y} * {z})\": [3, 5, 7], \n",
    "\"({x}*{y}*{z})\": [0, 2, 4], \n",
    "\"(({x}-{y})*{z})\": [4, 6, 9]}\n",
    "\n",
    "words_to_n = {convert_to_words(str(i)): i for i in range(args.max_n + 1)}\n",
    "\n",
    "new_intervention = copy.deepcopy(intervention)\n",
    "few_shot_result = tokenizer.decode(new_intervention.base_string_tok[0][new_intervention.len_few_shots - 3])[1:]\n",
    "few_shot_result_int = words_to_n[few_shot_result]\n",
    "new_result = 10\n",
    "new_result_string = ' ' + str(new_result)\n",
    "print(new_result_string)\n",
    "new_result_enc = tokenizer.encode(new_result_string)[0]\n",
    "print(new_result_enc)\n",
    "new_intervention.base_string_tok[0][new_intervention.len_few_shots - 3] = new_result_enc\n",
    "new_few_shot_string = tokenizer.decode(new_intervention.base_string_tok[0][:intervention.len_few_shots-1])\n",
    "print(new_few_shot_string)\n",
    "position_operands = equation_position_operands[intervention.equation]\n",
    "print(position_operands)\n",
    "symbols = [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\", \"zeta\", \n",
    "           \"eta\", \"theta\", \"iota\", \"number\", \"result\", \n",
    "           \"x\", \"y\", \"z\", \"a\", \"b\", \"c\"]\n",
    "for pos in position_operands:\n",
    "     new_operand_str = random.choice(symbols)\n",
    "     new_operand_str = ' ' + new_operand_str\n",
    "     new_operand_enc = tokenizer.encode(new_operand_str)[0]\n",
    "     new_intervention.base_string_tok[0][pos] = new_operand_enc\n",
    "print(tokenizer.decode(new_intervention.base_string_tok[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arithmetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
